# 数据分区

了解不同的数据分区模型及其优缺点。

## 为什么需要分区数据？

数据是任何组织的资产。逐渐增加的数据和并发读写流量对传统数据库造成了可伸缩性压力。因此，延迟和吞吐量受到影响。传统数据库由于其诸如范围查询、二级索引和具有ACID属性的事务等特性而具有吸引力。

但在某个时刻，单个基于节点的数据库已经无法处理负载。我们可能需要将数据分布在多个节点上，但仍要导出关系数据库的所有好处。实践证明，在分布式数据库上提供单节点数据库类似的属性是具有挑战性的。

其中一种解决方案是将数据移动到类似NoSQL的系统中。然而，历史上的代码库及其与传统数据库的紧密联系使其成为一个昂贵的问题。

组织机构可以使用第三方解决方案来扩展传统数据库。但是，集成第三方解决方案通常是复杂的。更重要的是，有丰富的机会可以针对手头的特定问题进行优化，并获得比通用解决方案更好的性能。

数据分区（或分片）使我们能够使用多个节点，其中每个节点管理整个数据的一部分。为了处理越来越多的查询率和数据量，我们努力实现平衡的分区和平衡的读写负载。

我们将在本课程中讨论不同的数据分区方式、相关的挑战以及它们的解决方案。

![QQ截图20230407113920](/img/09-Databases/QQ截图20230407113920.png)

带有两个分区的数据库，用于分配数据和相关的读写负载

## 分片

为了在多个节点之间分配负载，我们需要通过称为**分区**或**分片**的现象对数据进行分区。在这种方法中，我们将一个大的数据集拆分成存储在网络上不同节点上的较小数据块。

分区必须是平衡的，以便每个分区接收大致相同数量的数据。如果分区不平衡，大部分查询将落入少数分区。负载过重的分区将创建系统瓶颈。分区的效果将受到影响，因为大部分数据检索查询将被发送到负载过重的节点。这些分区被称为热点。通常，我们使用以下两种方式对数据进行分区：

- 垂直分区
- 水平分区

### 垂直分区

我们可以将不同的表放置在不同的数据库实例中，这些实例可能在不同的物理服务器上运行。我们可能会将一个表分成多个表，使得一些列在一个表中，而其余列在其他表中。如果存在多个表之间的连接，则应小心处理。我们可能希望在一个分片上保留这样的表。

通常，**垂直分区**用于加速从具有非常宽的文本或二进制大对象（blob）列的表中检索数据。在这种情况下，具有大文本或blob的列被拆分成不同的表。

如下图所示，`Employee`表被分成两个表：简化的`Employee`表和`EmployeePicture`表。`EmployeePicture`表仅有两列，即`EmployeeID`和`Picture`，与原始表分开。此外，`Employee`表的主键`EmpoloyeeID`被添加到两个分区表中。这使得数据读取和写入更加容易，并且表的重组被有效地执行。

垂直分区具有其复杂性，更易于手动分区，干系人可以仔细决定如何对数据进行划分。相比之下，水平分区适用于动态条件下的自动化。

![QQ截图20230407113933](/img/09-Databases/QQ截图20230407113933.png)

垂直分区### 水平分片

有时候，数据库中的一些表变得太大，影响读写延迟。**水平分片**或分区用于通过按行切分数据将一个表划分为多个表，如下一节中的图所示。原始表的每个分区分布在不同的数据库服务器上，被称为**分片**。通常有两种可用的策略：

- 基于键范围的分片
- 基于哈希的分片

#### 基于键范围的分片

在**基于键范围的分片**中，每个分区被分配了一组连续的键范围。在下面的图中，使用`Customer_Id`作为分区键对`Invoice`表执行了水平分区。两个不同颜色的表代表分区。

![QQ截图20230407113944](/img/09-Databases/QQ截图20230407113944.png)

水平分区

有时候，数据库由多个由外键关系绑定的表组成。在这种情况下，使用相同的分区键在所有表之间执行水平分片。属于相同分区键的表（或子表）分布在同一个数据库分片中。下图显示了具有相同分区键的多个表放置在单个数据库分片中：

![QQ截图20230407114017](/img/09-Databases/QQ截图20230407114017.png)

一组表的水平分区

多表分片中使用的基本设计技术如下：

- 在`Customer`映射表中有一个分区键。该表存在于每个分片中，存储用于该分片的分区键。应用程序通过从所有分片中读取此表来创建分区键和数据库分片之间的映射逻辑，以使映射高效。有时候，应用程序使用先进的算法来确定属于特定分片的分区键的位置。
- 在所有其他表中，将分区键列`Customer_Id`复制为数据隔离点。引入存储的代价和定位所需分片的代价之间存在权衡。除此之外，它有助于将数据和工作负载分配到不同的数据库分片中。数据路由逻辑使用应用程序层中的分区键将查询映射到特定的数据库分片。
- 主键在所有数据库分片中都是唯一的，以避免数据迁移过程中的键冲突及 在联机分析处理（OLAP）环境中合并数据时的问题。
- 列`Creation_date`作为数据一致性点，假设所有节点上的时钟同步。当需要时，该列用作从所有数据库分片中合并数据到全局视图时的标准。

##### 优点

使用这种方法，基于范围查询的方案易于实现。可以使用分区键执行范围查询，并将其按顺序保存在分区中。

##### 缺点

不能使用除分区键以外的键执行范围查询。如果选择的键不当，某些节点可能会由于流量不均匀而存储更多数据。

#### 基于哈希的分片

**基于哈希的分片**利用哈希函数对属性进行哈希运算，并根据执行分片的属性产生不同的值。其主要概念是在键上使用哈希函数来获取哈希值，然后对分区数进行取模。一旦找到了适当的键哈希函数，我们就可以为每个分区分配一组哈希范围（而不是键范围）。任何哈希出现在该范围内的主键将存储在该分区中。

```在下面的插图中，我们使用一个哈希函数：```节点数为四。我们通过检查每个键的模分配键到节点。具有模值等于2的键将分配给节点2。具有模值为1的键分配给节点1。具有模值为3的键分配给节点3。因为没有模值为0的键，节点0为空。```![QQ截图20230407114628](/img/09-Databases/QQ截图20230407114628.png)![QQ截图20230407114028](/img/09-Databases/QQ截图20230407114028.png)基于哈希的分片##### 优点- 键均匀分布在所有节点之间。##### 缺点- 无法进行范围查询。键将分散在所有分区中。> **备注：**每个数据库有多少个分片？>> 根据经验，我们可以确定每个节点可以提供哪些可接受的性能。这可以帮助我们找出我们希望将多少数据放在任何一个节点上的最大值。例如，如果我们发现我们可以在一个节点上放置最多50 GB的数据，那么我们有以下内容：>>数据库大小==10 TB>>单个分片的大小==50 GB>>该数据库应分布的分片数==10 TB / 50 GB == 200#### 一致性哈希**一致性哈希**为分布式哈希表中的每个服务器或项分配了一个抽象环上的位置，称为环，而不考虑表中的服务器数。这允许服务器和对象在不影响系统整体性能的情况下进行扩展。##### 一致性哈希的优点- 水平扩展非常容易。- 它增加了应用程序的吞吐量并提高了延迟。##### 一致性哈希的缺点- 在环中随机分配节点可能会导致不均匀的分布。### 重新平衡分区由于许多原因，包括以下原因，查询负载可能在节点之间不平衡：- 数据分布不平均。- 一个分区上有太大的负载。- 查询流量增加，我们需要添加更多节点来保持相应能力。我们可以采用以下策略来重新平衡分区。#### 避免哈希mod n```通常，我们避免对分区进行键的哈希处理（我们之前使用此方案简单地解释了哈希的概念）。在哈希modn的情况下添加或删除节点的问题是每个节点的分区编号都会更改，并且会移动大量数据。例如，假设我们有1235hash（key）= 1235。如果一开始有五个节点，键将从节点1开始（）。现在，如果添加了一个新节点，键将必须移动到节点6（），以此类推。这将使从一个节点到另一个节点的键的移动成本高昂。```![QQ截图20230407114753](/img/09-Databases/QQ截图20230407114753.png)#### 固定分区数在这种方法中，设置数据库时预先确定要创建的分区数量。我们创建比节点数更多的分区并将这些分区分配给节点。因此，当系统添加新节点时，它可以从现有节点中获取一些分区，直到分区平均分配。这种方法存在一个缺点。由于所有分区都包含总数据的一小部分，因此随着总数据量的增加，每个分区的大小也会增加。如果分区非常小，则会产生过多的开销，因为我们可能需要创建大量小型分区，每个分区都会花费一些开销。如果分区非常大，则从节点故障中重新平衡节点和恢复将会很昂贵。选择正确的分区数量非常重要。Elasticsearch，Riak等使用固定数量的分区。#### 动态分区## 分区策略

在这种方法中，当一个分区的大小达到阈值时，它被平均分成两个分区。其中一个分区分配给一个节点，另一个分区分配给另一个节点。这样，负载被平均分配。分区的数量适应于整体数据量，这是动态分区的优点。但是，这种方法也有一个缺点。在服务读写时应用动态再平衡是困难的。这种方法在 HBase 和 MongoDB 中使用。

#### 按节点比例分区

在这种方法中，分区的数量与节点数量成比例，这意味着每个节点有固定的分区。在较早的方法中，分区的数量取决于数据集的大小。但在这里，情况并非如此。当节点数量保持恒定时，每个分区的大小会根据数据集大小增加。然而，随着节点数量增加，分区会缩小。当一个新节点进入网络时，它会在当前的一定数量的分区中随机分裂，然后取一半分区，另一半则不变。这可能会导致不公平的分裂。这种方法是 Cassandra 和 Ketama 使用的方法。

值得思考

###### 问题

是谁执行再平衡？自动还是手动？

隐藏答案

执行再平衡有两种方式：自动和手动。在自动再平衡中，没有管理员，系统决定何时执行分区以及何时将数据从一个节点移动到另一个节点。在手动再平衡中，管理员确定何时以及如何执行分区。组织根据自己的需要进行再平衡。有些使用自动再平衡，有些使用手动再平衡。

### 分区和二级索引

我们已经讨论了基于键值数据模型的分区方案，其中记录是通过主键检索的。但是，如果我们必须通过二级索引访问记录怎么办呢？二级索引是没有通过主键进行标识的记录，但是只是搜索某个值的方式。例如，上面的水平分区的插图包含了客户表，搜索所有创建年份相同的客户。

我们可以通过以下方式使用二级索引进行分区。

#### 按文档分区二级索引

在这种索引方法中，每个分区都是完全独立的。每个分区都有其自己的二级索引，仅涵盖该分区中的文档。它不关心其他分区中持有的数据。如果我们想向数据库写入任何内容，我们只需要处理包含正在写入的文档 ID 的分区。它也被称为本地索引。在下面的插图中，有三个分区，每个分区都有其自己的身份和数据。如果我们想获得所有名为 `John` 的客户 ID，我们必须从所有分区请求。

然而，这种在二级索引上的查询可能是昂贵的。由于受制于效能较差的分区的延迟，读取查询延迟可能会增加。

![QQ截图20230407114041](/img/09-Databases/QQ截图20230407114041.png)

按文档分区二级索引

#### 按项分区二级索引

与为每个分区创建一个二级索引（本地索引）不同，我们可以为从所有分区中涵盖数据的二级项创建一个全局索引。在下图中，我们在名称上创建索引（我们正在分区的术语）并将所有名称索引存储在单独的节点上。要获取所有名为“John”的客户的`cust_id`，我们必须确定我们的术语索引位于哪里。`索引0`包含所有名字以“A”到“M”开头的客户。`索引1`包括所有以“N”到“Z”开头的客户。由于`John`位于`索引0`中，因此我们从`索引0`中获取具有名称`John`的`cust_id`列表。按词分区的二级索引比按文档分区的二级索引更具有阅读效率。这是因为它只访问包含该术语的分区。但是，在此方法中，单个写入会影响多个分区，使方法写密集且复杂。

![QQ截图20230407114051](/img/09-Databases/QQ截图20230407114051.png)

按术语分区的二级索引

## 请求路由

我们已经了解了如何分区我们的数据。但是，这里有一个问题：当客户端发出请求时，客户端如何知道连接哪个节点？在重新平衡后，将分区分配给节点的方式会有所不同。如果我们要读取特定的键，我们如何知道我们需要连接到哪个IP地址才能读取？

这个问题也称为**服务发现**。以下是解决这个问题的几种方法：

- 允许客户端请求网络中的任何节点。如果该节点不包含所请求的数据，则将该请求转发到包含相关数据的节点。
- 第二种方法包含一个路由层。所有请求首先转发到路由层，它确定要连接的节点以满足请求。
- 客户端已经拥有与分区相关的信息，以及哪个分区连接到哪个节点。因此，它们可以直接联系包含所需数据的节点。

在所有这些方法中，主要的挑战是确定这些组件如何了解节点分区的更新。

### ZooKeeper

为了跟踪集群中的更改，许多分布式数据系统需要单独的管理服务器，例如ZooKeeper。**ZooKeeper**跟踪网络中的所有映射，每个节点都连接到ZooKeeper以获取信息。每当分区发生更改或添加或删除节点时，ZooKeeper都会更新并通知路由层有关更改的信息。HBase，Kafka和SolrCloud使用ZooKeeper。

## 结论

对于所有当前的分布式系统，分区已成为标准协议。由于系统包含越来越多的数据，对数据进行分区是有道理的，因为它加快了写入和读取速度。 它提高了系统的可用性，可扩展性和性能。