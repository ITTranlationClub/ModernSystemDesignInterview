---
typora-root-url: ..
---

# 数据分区

学习数据分区模型及其优缺点。

## 为什么要分区数据？

数据是任何组织的资产。随着数据和并发读 / 写流量增加，传统数据库扩展性的压力也增加，导致响应延迟和吞吐量受到影响。传统数据库由于其区间查询、二级索引和具有ACID属性的事务等特性而具有吸引力。

在某个点上，单个基于节点的数据库不足以处理负载。我们可能需要将数据分布在许多节点上，但仍要导出关系型数据库的所有良好属性。实践中，提供类似单节点数据库的属性的分布式数据库已被证明是具有挑战性的。

解决方案之一是将数据移动到类似NoSQL的系统。但是，历史代码库及其与传统数据库的密切关系使其成为一个昂贵的问题。

组织可以通过使用第三方解决方案来扩展传统数据库。但是，集成第三方解决方案通常非常复杂。更重要的是，有丰富的机会针对特定问题进行优化，并获得比通用解决方案更好的性能。

数据分区（或分片）使我们能够使用多个节点，其中每个节点管理整个数据的一部分。为了处理不断增加的查询率和数据量，我们努力实现平衡的分区和平衡的读 / 写负载。

在本课程中，我们将讨论分区数据、相关挑战及其解决方案的不同方法。

![QQ截图20230407113920](/img/09-Databases/QQ截图20230407113920.png)

通过两个分区来分发数据和相关的读写负载的数据库

## 分片

为了在多个节点之间分配负载，我们需要通过称为**分片**或**分区**的现象对数据进行分区。在此方法中，我们将大型数据集分成存储在网络上不同节点上的较小数据块。

分区必须平衡，以便每个分区接收大约相同数量的数据。如果分区不平衡，则大多数查询将落入几个分区中。负载较重的分区将创建系统瓶颈。如果将大量的数据检索查询发送到负载高的分区，则会损害分区的有效性。这些分区称为热点。通常，我们使用以下两种方式来分片数据：

- 垂直分片
- 水平分片

### 垂直分片

我们可以将不同的表放入各种数据库实例中，这些实例可能在不同的物理服务器上运行。我们可以将表分成多个表，以便某些列位于一个表中，而其余列位于另一个表中。如果存在多个表之间的连接，则应小心处理。我们可能希望将这样的表放在一个分片上。

通常，**垂直分片**用于增加从由包含非常宽的文本或二进制大对象（BLOB）列的表检索数据的速度。在这种情况下，具有大型文本或BLOB的列被拆分到不同的表中。

如下图所示，`Employee`表被划分为两个表：精简的`Employee`表和`EmployeePicture`表。 `EmployeePicture`表仅有两列，即从原始表中分离的`EmployeID`和`Picture`。此外，`Employee`表的主键`EmpoloyeeID`被添加到两个分区表中。这使得数据读写更加容易，并且可以高效地执行表的重建。

垂直分区具有其复杂性，并且更适合手动分区，其中利益相关者会仔细决定如何分区数据。相比之下，水平分片更适合在动态情况下自动化。

![QQ截图20230407113933](/img/09-Databases/QQ截图20230407113933.png)

垂直分区### 水平分片

有时候，数据库中的某些表格变得太大，影响读写延迟。**水平分片**或分区是将一张表格按行拆成多个表格的方法，如下一节中的图所示。原始表格的每个分区都分布在数据库服务器上，称为**分片**。通常有两种可用的策略：

- 基于键范围的分片
- 基于哈希的分片

#### 基于键范围的分片

在**基于键范围的分片**中，每个分区都被分配了一段连续的键区间。在下图中，使用`Customer_Id`作为分区键对`Invoice`表进行水平分割，显示了两个不同颜色的表格表示分区。

![QQ截图20230407113944](/img/09-Databases/QQ截图20230407113944.png)

水平分片

有时候，数据库由许多由外键关系绑定的表格组成。在这种情况下，在一个关系中的所有表格上使用相同的分区键进行水平分片。属于相同分区键的表格（或子表格）分布在一个数据库分片中。下图显示了将具有相同分区键的多个表格放置在单个数据库分片中：

![QQ截图20230407114017](/img/09-Databases/QQ截图20230407114017.png)

在一组表格上的水平分片

在多表分片中使用的基本设计技术如下：

- `Customer`映射表格中有一个分区键，该表格驻留在每个分片中并存储在该分片中使用的分区键。应用程序通过从所有分片中读取此表格来创建分区键和数据库分片之间的映射逻辑，以使映射更有效率。有时，应用程序使用高级算法来确定属于特定分片的分区键的位置。
- 分区键列`Customer_Id`作为数据隔离点在所有其他表格中进行复制。它具有在增加存储和有效定位所需分片之间的权衡。除此之外，它有助于将数据和工作量分布到不同的数据库分片中。数据路由逻辑在应用程序层使用分区键将指定为数据库分片的查询进行映射。
- 主键在所有数据库分片中均为唯一，以避免在数据迁移和在线分析处理（OLAP）环境中合并数据时出现键冲突。
- 列`Creation_date`作为数据一致性点，假设所有节点的时钟都同步。当必要时，该列用作从所有数据库分片中合并数据到全局视图的标准。

##### 优点

使用此方法，基于范围查询的方案易于实现。可以使用分区键执行范围查询，并且可以将这些查询保留在分区中，以排序方式排序。

##### 缺点

无法使用除分区键以外的键执行范围查询。如果键没有选择正确，则某些节点可能必须存储更多的数据，因为流量分布不均匀。

#### 基于哈希的分片

**基于哈希的分片**使用类似哈希函数的属性，根据进行分片的属性生成不同的值。主要想法是在关键词上使用哈希函数以获得哈希值，然后对分区数进行取模。一旦我们找到了适合关键词的哈希函数，就可以给每个分区分配一段哈希值范围（而不是一段键范围）。任何哈希发生在该范围内的键将保留在该分区中。

![QQ截图20230407114628](/img/09-Databases/QQ截图20230407114628.png)

![QQ截图20230407114028](/img/09-Databases/QQ截图20230407114028.png)

基于哈希的分片

##### 优点

节点上的密钥均匀分布。

##### 缺点

使用此技术无法执行范围查询。键将分散在所有分区中。> **注意：**每个数据库应该有多少个分片？>> 根据实际情况，我们可以确定每个节点可提供的可接受性能数量。这可以帮助我们找出希望在任何一个节点上保留的最大数据量。例如，如果我们发现我们最多可以在一个节点上放置50GB的数据，则有以下内容：>> 数据库大小 == 10 TB>> 单个分片的大小 == 50 GB>> 数据库应分布在的分片数量 == 10 TB / 50 GB == 200个分片

#### 一致性哈希

**一致性哈希**会为分布式哈希表中的每个服务器或项分配一个位于抽象环形上的位置，称为环，而不考虑表中服务器的数量。这使得服务器和对象可以在不影响系统整体性能的情况下进行扩展。##### 一致性哈希的优点- 水平扩展很容易实现。- 它增加了应用程序的吞吐量并改善了响应时间。##### 一致性哈希的缺点- 在环中随机分配节点可能会导致分布不均匀。### 重新平衡分区由于许多原因，包括以下原因，查询负载可能在节点之间不均衡：- 数据分布不均匀。- 单个分区上的负载过多。- 查询流量增加，我们需要添加更多节点来跟上工作。我们可以采用以下策略重新平衡分区。#### 避免哈希模n![QQ截图20230407114753](/img/09-Databases/QQ截图20230407114753.png)
$$

$$


#### 固定数量的分区在这种方法中，创建分区的数量在设置数据库时就已经固定了。我们创建了比节点多的分区并将这些分区分配给节点。因此，当新节点添加到系统中时，它可以从现有节点中获取一些分区，直到分区平均分配为止。这种方法的缺点是每个分区的大小随着群集中数据的总量而增长，因为所有分区都包含总数据的一小部分。如果分区非常小，将导致过多的开销，因为我们可能必须创建大量的小分区，每个分区都会产生一些开销。如果分区非常大，重新平衡节点并从节点故障中恢复将是昂贵的。选择正确的分区数量非常重要。固定数量的分区在Elasticsearch、Riak等系统中使用。#### 动态分区在此方法中，当分区大小达到阈值时，它将被平均分成两个分区。两个分区中的一个分配给一个节点，另一个分配给另一个节点。通过这种方式，负载被平均分配。分区数随着总体数据量而变化，这是动态分区的优势。但是，这种方法的缺点是，在读取和写入时很难应用动态重平衡。HBase和MongoDB使用此方法。#### 根据节点比例分区在此方法中，分区的数量与节点的数量成比例，这意味着每个节点都有固定的分区。在之前的方法中，分区的数量取决于数据集的大小，但在此情况下不是这样。当节点数量保持不变时，每个分区的大小随着数据集大小而增加。但是，随着节点数量的增加，分区会变小。当新节点进入网络时，它会随机分割一定数量的当前分区，然后将其中一半分配给一个节点，另一半保持不变。这可能会导致不公平的分割。Cassandra和Ketama使用此方法。值得思考###### 问题由谁执行重新平衡？是自动还是手动？隐藏答案有两种方法可以执行重新平衡：自动和手动。在 **自动重新平衡** 中，没有管理员。系统决定何时执行分区以及何时将数据从一个节点移动到另一个节点。在 **手动重新平衡** 中，管理员确定何时以及如何执行分区。组织根据自身需求执行重新平衡。有些人使用自动重新平衡，有些人使用手动重新平衡。

### 分区和辅助索引

我们已经讨论了基于键值数据模型的分区方案，其中记录通过主键检索。但是，如果我们需要通过辅助索引访问记录怎么办？辅助索引是没有通过主键标识的记录，只是一种搜索某个值的方式。例如，上面的[水平分区示例](https://www.educative.io/collection/page/10370001/4941429335392256/6254160546103296#key-range-based-sharding)包含客户表，搜索所有创建年份相同的客户。

我们可以使用以下方式通过辅助索引进行分区。

#### 按文档分区辅助索引

在此索引方法中，每个分区都是完全独立的。每个分区都有其自己的辅助索引，仅涵盖该分区中的文档。它不关心其他分区中保存的数据。如果我们想要写入数据库中的任何内容，我们只需要处理包含正在写入的文档ID的分区。它也称为本地索引。在下面的示例中，有三个分区，每个分区都有自己的标识和数据。如果我们想要获取所有名为`John`的客户ID，我们必须从所有分区中请求。

然而，此类辅助索引查询可能很昂贵。由于受到性能较差的分区的延迟限制，读取查询延迟可能会增加。

![QQ截图20230407114041](/img/09-Databases/QQ截图20230407114041.png)

#### 按术语分区辅助索引

我们可以为辅助术语创建一个包含所有分区（全局索引）数据的全局索引，而不是针对每个分区创建一个辅助索引（本地索引）。

在下面的示例中，我们在名称上创建索引（我们正在分区的术语），并在独立的节点上存储所有名称的索引。要获取名为`John`的所有客户的`cust_id`，我们必须确定我们的术语索引位于何处。`index 0`包含名称以“A”到“M”开头的所有客户。`index 1`包括所有名称以“N”到“Z”开头的客户。因为`John`属于`index 0`，所以我们从`index 0`获取名为`John`的`cust_id`列表。

按术语分区辅助索引比按文档分区辅助索引更具读取效率。这是因为它仅访问包含术语的分区。但是，此方法中的单个写入影响多个分区，使该方法变得写入密集和复杂。

![QQ截图20230407114051](/img/09-Databases/QQ截图20230407114051.png)

## 请求路由

我们已经学习了如何分区我们的数据。但是，这里出现一个问题：在发出请求时，客户端如何知道连接到哪个节点？在重新平衡后，将分区分配给节点的情况会有所不同。如果我们想要读取特定的键，如何知道需要连接到哪个IP地址进行读取？

这个问题也被称为**服务发现**。以下是解决此问题的几种方法：

- 允许客户端请求网络中的任何节点。如果该节点不包含所请求的数据，则它将该请求转发到包含相关数据的节点。
- 第二种方法包含一个路由层。所有请求首先被转发到路由层，它确定连接到哪个节点以满足请求。客户端已经具备与分区相关且知道哪个分区与哪个节点连接的信息，因此，他们可以直接联系包含所需数据的节点。

在所有这些方法中，主要的挑战是确定这些组件如何知道节点分区的更新。### ZooKeeper为了跟踪集群中的更改，许多分布式数据系统需要一个独立的管理服务器，例如ZooKeeper。**ZooKeeper**跟踪网络中的所有映射，每个节点都连接到ZooKeeper以获取信息。每当分区发生变化或添加或删除节点时，ZooKeeper都会进行更新，并通知路由层进行更改。HBase，Kafka和SolrCloud使用ZooKeeper。## 结论对于所有当前的分布式系统，分区已成为标准协议。由于系统包含越来越多的数据，分区数据是有意义的，因为它可以加快写入和读取的速度。它提高了系统的可用性、可伸缩性和性能。